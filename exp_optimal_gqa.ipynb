{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c07f44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4dc0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl kaggle data - https://www.kaggle.com/datasets/kutayahin/stackoverflow-programming-questions-2020-2025\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "dir = kagglehub.dataset_download('dhruvildave/en-fr-translation-dataset')\n",
    "filename = 'en-fr.csv'\n",
    "filepath = os.path.join(dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82525129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(12)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(12)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a630bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "CSV_PATH = filepath\n",
    "BATCH_SIZE = 8\n",
    "N = 2048\n",
    "N_TEST = 2048\n",
    "D_MODEL = 512\n",
    "D_V = 64\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=N) \n",
    "\n",
    "\n",
    "# extract raw data\n",
    "def extract(batch_size: int = BATCH_SIZE, csv_path: str = CSV_PATH) -> tuple[list[str], list[str]]:\n",
    "    chunk_iterator = pd.read_csv(\n",
    "        csv_path, \n",
    "        chunksize=batch_size * 10, # secure enough data\n",
    "        header=None,\n",
    "        names=['en','fr'],\n",
    "        usecols=['en', 'fr'],\n",
    "        on_bad_lines='skip',\n",
    "        na_filter=True,\n",
    "        skiprows=[0, 1, 2, 3, 4, 5], # drop rows w/ irrelevant data\n",
    "        skip_blank_lines=True,\n",
    "    )\n",
    "    first_chunk = next(chunk_iterator)\n",
    "    raw_en = first_chunk['en'].astype(str).tolist()\n",
    "    raw_fr = first_chunk['fr'].astype(str).tolist()\n",
    "    raw_en_sentences, raw_fr_sentences = list(), list()\n",
    "    for i, item in enumerate(raw_en):\n",
    "        if len(item.split(' ')) > 1 and i < batch_size: \n",
    "            raw_en_sentences.append(item)\n",
    "            raw_fr_sentences.append(raw_fr[i])\n",
    "    return raw_en_sentences, raw_fr_sentences\n",
    "\n",
    "en_raw, fr_raw = extract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "723cf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention modules - standard, mha, mqa, gqa\n",
    "\n",
    "from typing import Optional\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, d_V: int = 64, projection: bool = True) -> None: # d_model - dim of the model input layer, d_v - dim of value vector\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_V = d_V\n",
    "        self.projection = projection\n",
    "        self.scaling_factor = 1 / t.sqrt(t.tensor(self.d_V, requires_grad=False))\n",
    "        self.query = nn.Linear(in_features=self.d_model, out_features=self.d_V, bias=True)\n",
    "        self.key = nn.Linear(in_features=self.d_model, out_features=self.d_V, bias=True)\n",
    "        self.value = nn.Linear(in_features=self.d_model, out_features=self.d_V, bias=True)\n",
    "        self.output_proj = nn.Linear(in_features=self.d_V, out_features=self.d_model, bias=False) # output projection layer\n",
    "\n",
    "    def self_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: Optional[t.BoolTensor] = None) -> t.Tensor:       \n",
    "        K_T = t.transpose(K, -1, -2)  # [b, N, D]\n",
    "        S = t.matmul(Q, K_T) * self.scaling_factor  # attention score\n",
    "        if mask is not None: S = t.masked_fill(S, mask==0, -t.inf) # mask (if any)\n",
    "        A = t.softmax(S, dim=-1) # attention weight\n",
    "        Z = t.matmul(A, V)  # context vector\n",
    "        return Z\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask: Optional[t.BoolTensor] = None) -> t.Tensor:\n",
    "        Q = self.query(x) # [b, N, D_V]\n",
    "        K = self.key(x) # [b, N, D_V]\n",
    "        V = self.value(x) # [b, N, D_V]\n",
    "        Z = self.self_attention(Q, K, V, mask=mask) # [b, N, D_V]\n",
    "        O = self.output_proj(Z) if self.projection else Z # [b, N, d_model] \n",
    "        return O\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, d_V: int = 64, H: int = 8) -> None: # H: total heads \n",
    "        super().__init__()\n",
    "        # input features: H * d_V. output features: d_model\n",
    "        self.proj = nn.Linear(in_features=H * d_V, out_features=d_model, bias=False) \n",
    "        self.multihead = nn.ModuleList([StandardAttention(d_model, d_V, False) for _ in range(H)])\n",
    "    \n",
    "    def forward(self, x: t.Tensor, mask: Optional[t.BoolTensor] = None) -> t.Tensor:\n",
    "        Z = t.cat([head(x, mask) for head in self.multihead], dim=2) \n",
    "        O = self.proj(Z)\n",
    "        return O\n",
    "\n",
    "\n",
    "class MQA(StandardAttention):\n",
    "    def __init__(self, d_model: int = 512, d_V: int = 64, n_queries: int = 8) -> None:\n",
    "        super().__init__(d_model, d_V)\n",
    "        self.n_queries = n_queries\n",
    "        self.proj = nn.Linear(in_features=d_V * n_queries, out_features=d_model, bias=False) \n",
    "        delattr(self, 'query') # remove inherited query\n",
    "\n",
    "        self.queries = nn.ModuleList([nn.Linear(in_features=d_model, out_features=d_V, bias=True) for _ in range(n_queries)])\n",
    "        self.key = nn.Linear(in_features=d_model, out_features=d_V, bias=True)\n",
    "        self.value = nn.Linear(in_features=d_model, out_features=d_V, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x: t.Tensor, mask: Optional[t.BoolTensor] = None) -> t.Tensor:\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        Z = t.cat([self.self_attention(query(x), K, V, mask) for query in self.queries], dim=2)\n",
    "        O = self.proj(Z)\n",
    "        return O\n",
    "\n",
    "\n",
    "class GQA(StandardAttention):\n",
    "    def __init__(self, d_model: int = 512, d_V: int = 64, n_groups: int = 4, n_queries: int = 2) -> None: # n_queries (for each group\n",
    "        super().__init__(d_model, d_V)\n",
    "        delattr(self, 'query')\n",
    "        delattr(self, 'key')\n",
    "        delattr(self, 'value')\n",
    "        self.groups = nn.ModuleList([MQA(d_model=d_model, d_V=d_V, n_queries=n_queries) for _ in range(n_groups)])\n",
    "        self.proj = nn.Linear(in_features=d_model * n_groups, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask: Optional[t.BoolTensor] = None) -> t.Tensor:\n",
    "        Z = t.cat([head(x, mask) for head in self.groups], dim=2)\n",
    "        O = self.proj(Z)\n",
    "        return O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ef55de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder module + positional encoding\n",
    "import math\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # compute the positional encodings (pe) once in log space\n",
    "        pe = t.zeros(max_len, d_model)\n",
    "        position = t.arange(0, max_len, dtype=t.float).unsqueeze(1)\n",
    "        div_term = t.exp(t.arange(0, d_model, 2).float() * ( - math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = t.sin(position * div_term)\n",
    "        pe[:, 1::2] = t.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # shape - (1, max_len, d_model)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "  \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :] # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention_module: StandardAttention | MHA | MQA | GQA, d_model: int):\n",
    "        # kwargs for attn module - options { d_model, d_V, H, n_groups, n_queries }\n",
    "        super().__init__()\n",
    "\n",
    "        # attn block\n",
    "        self.norm_attn = nn.LayerNorm(d_model)\n",
    "        self.attn = attention_module\n",
    "        self.dropout_attn = nn.Dropout(0.1)\n",
    "\n",
    "        # ffn block\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model * 4), nn.ReLU(), nn.Linear(d_model * 4, d_model))\n",
    "        self.norm_ffn = nn.LayerNorm(d_model)\n",
    "        self.dropout_ffn = nn.Dropout(0.1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        # attention block\n",
    "        norm_x = self.norm_attn(x)\n",
    "        O = self.attn(norm_x)\n",
    "        x = x + self.dropout_attn(O)\n",
    "\n",
    "        # ffn block\n",
    "        norm_x = self.norm_ffn(x)\n",
    "        ffn_out = self.ffn(norm_x)\n",
    "        output = x + self.dropout_ffn(ffn_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a91908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode module\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, attention_module: StandardAttention | MHA | MQA | GQA, d_model: int):\n",
    "        super().__init__()\n",
    "        # self attn block\n",
    "        self.self_attn = attention_module # decoder self-attn (same as encoder self-attention)\n",
    "        self.norm_self_attn = nn.LayerNorm(d_model)\n",
    "        self.dropout_self_attn = nn.Dropout(0.1)\n",
    "\n",
    "        # cross attn block\n",
    "        self.cross_attn = MHA(d_model) # encoder-decoder attn\n",
    "        self.norm_cross_attn = nn.LayerNorm(d_model)\n",
    "        self.dropout_cross_attn = nn.Dropout(0.1)\n",
    "\n",
    "        # ffn block\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model * 4), nn.ReLU(), nn.Linear(d_model * 4, d_model))      \n",
    "        self.norm_ffn = nn.LayerNorm(d_model)\n",
    "        self.dropout_ffn = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, tgt: t.Tensor, mem: t.Tensor) -> t.Tensor:  \n",
    "        # self attn (q = k = v = tgt)\n",
    "        norm_tgt_self_attn = self.norm_self_attn(tgt)\n",
    "        self_attn_out = self.self_attn(norm_tgt_self_attn)\n",
    "        tgt = tgt + self.dropout_self_attn(self_attn_out)\n",
    "\n",
    "        # encoder-decoder cross-attn (q = tgt, k = v = mem)\n",
    "        norm_tgt_cross_attn = self.norm_cross_attn(tgt)\n",
    "        cross_attn_out = self.cross_attn(norm_tgt_cross_attn) \n",
    "        tgt = tgt + self.dropout_cross_attn(cross_attn_out)\n",
    "\n",
    "        # ffn\n",
    "        norm_tgt = self.norm_ffn(tgt)\n",
    "        ffn_out = self.ffn(norm_tgt)\n",
    "        output = tgt + self.dropout_ffn(ffn_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b646584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... model config: H=8, D_MODEL=512, D_V=64 ...\n",
      "... initial gqa config: G=4, D=2 ...\n",
      "... calculating similarity matrix for 8 heads ...\n",
      "... testing possible group sizes D's ...\n",
      "   -> G=8 (D=1): mha with max groups. score = 0.0000\n",
      "   -> G=4 (D=2): Best Sim Score (SA) = 0.9836\n",
      "   -> G=2 (D=4): Best Sim Score (SA) = 0.9836\n",
      "   -> G=1 (D=8): mqa with a single group. score = 0.4219\n",
      "\n",
      "... optimal gqa structure found:\n",
      "  - optimal groups: 4\n",
      "  - optimal group size: 2\n",
      "  - maximum similarity score: 0.9836\n",
      "  - best grouping (head indices):\n",
      "      [[0, 3], [6, 7], [4, 5], [1, 2]]\n",
      "\n",
      "conclusion: the optimal grouping was found by simulated annealing.\n"
     ]
    }
   ],
   "source": [
    "# transformer\n",
    "\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention_module: StandardAttention | MHA | MQA | GQA,\n",
    "            d_model: int,\n",
    "            max_seq_len: int,\n",
    "            tokenizer,\n",
    "            device: t.device = DEVICE,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # device\n",
    "        self.device = device\n",
    "\n",
    "        # dim, model name\n",
    "        self.d_model = d_model\n",
    "        self.attention_module = attention_module\n",
    "        self.model_name = self.attention_module.__class__.__name__\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        # self.bos_token_id = self.tokenizer.bos_token_id if hasattr(self.tokenizer, 'bos_token_id') else self.eos_token_id\n",
    "        self.bos_token_id = self.eos_token_id\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # encoder\n",
    "        self.input_token_embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.positional_encoder = PositionalEncoding(d_model=self.d_model, max_len=max_seq_len)\n",
    "        self.dropout_encoder = nn.Dropout(0.1) # after embeddings\n",
    "        self.encoder = EncoderLayer(attention_module=attention_module, d_model=d_model)\n",
    "        \n",
    "        # decoder \n",
    "        self.target_token_embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.dropout_decoder = nn.Dropout(0.1)\n",
    "        self.decoder = DecoderLayer(attention_module=attention_module, d_model=d_model)\n",
    "        \n",
    "        # final linear/softmax head for logits\n",
    "        self.linear_head = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids: t.Tensor, Y_true: t.Tensor) -> tuple[t.Tensor, t.Tensor]:\n",
    "        ## encoder\n",
    "        # token embedding, scaling\n",
    "        input_tokens = self.input_token_embedding(input_ids) * math.sqrt(self.d_model)\n",
    "        # add positional encodings w/ dropout\n",
    "        input_tokens = self.dropout_encoder(self.positional_encoder(input_tokens)) \n",
    "        # forward pass\n",
    "        output_encoder = self.encoder(input_tokens)\n",
    "\n",
    "        ## decoder\n",
    "        # decoder input - Y_true shifted right to exclude the last item\n",
    "        tgt_input_tokens = Y_true[:, :-1]\n",
    "        # token embedding, scaling\n",
    "        tgt = self.target_token_embedding(tgt_input_tokens) * math.sqrt(self.d_model)\n",
    "        # add positional encodings w/ dropout\n",
    "        tgt = self.dropout_decoder(self.positional_encoder(tgt)) \n",
    "        # forward pass\n",
    "        output_decoder = self.decoder(tgt, output_encoder) \n",
    "\n",
    "        ## final output\n",
    "        # linear head to project D_MODEL to vocab size\n",
    "        logits = self.linear_head(output_decoder)\n",
    "        # target labels - Y_true shifted left (T1, T2, ..., Tn, EOS)\n",
    "        target_labels = Y_true[:, 1:] \n",
    "        return logits, target_labels\n",
    "    \n",
    "    \n",
    "    # performs autoregressive decoding (inf) to generate target sequence\n",
    "    def generate(self, input_ids: t.Tensor, max_length: int | None = None) -> t.Tensor:\n",
    "        self.eval()\n",
    "        B = input_ids.shape[0]\n",
    "        max_length = max_length if max_length is not None else self.max_seq_len\n",
    "\n",
    "        with t.no_grad():\n",
    "            input_tokens = self.input_token_embedding(input_ids) * math.sqrt(self.d_model)\n",
    "            input_tokens = self.dropout_encoder(self.positional_encoder(input_tokens)) \n",
    "            encoder_output = self.encoder(input_tokens)\n",
    "\n",
    "            # prepare the initial decoder input (bos token)\n",
    "            tgt_output = t.ones((B, 1), dtype=t.long, device=self.device) * self.bos_token_id\n",
    "            is_finished = t.zeros(B, dtype=t.bool, device=self.device)\n",
    "\n",
    "            # autoregressive decoding loop\n",
    "            for _ in range(max_length - 1): # max length -  N-1 steps after bos\n",
    "                tgt_emb = self.target_token_embedding(tgt_output) * math.sqrt(self.d_model) # decoder input = tgt_output\n",
    "                tgt_emb = self.positional_encoder(tgt_emb)\n",
    "                decoder_output = self.decoder(tgt_emb, encoder_output) \n",
    "                logits = self.linear_head(decoder_output[:, -1, :]) \n",
    "                \n",
    "                # get the predicted token (greedy decoding - max probability)\n",
    "                next_token_id = t.argmax(logits, dim=-1) # shape (B,)\n",
    "\n",
    "                # update the status (finish if a sequence hits eos)\n",
    "                is_finished = is_finished | (next_token_id == self.eos_token_id)\n",
    "                \n",
    "                # append the predicted token to the output sequence\n",
    "                tgt_output = t.cat([tgt_output, next_token_id.unsqueeze(1)], dim=-1)\n",
    "                \n",
    "                # stop if all sequences completed\n",
    "                if is_finished.all(): break\n",
    "        \n",
    "        # return the generated sequence including the bos and eos tokens\n",
    "        return tgt_output\n",
    "\n",
    "\n",
    "    def _orthogonal_transform(self, A: t.Tensor, B: t.Tensor) -> t.Tensor:\n",
    "        C = A @ B.transpose(-1, -2) \n",
    "        try:\n",
    "            U, _, V_h = t.linalg.svd(C)\n",
    "            Q = V_h.transpose(-1, -2) @ U.transpose(-1, -2)\n",
    "        except:\n",
    "            # fallback to identity matrix on svd failure\n",
    "            return t.eye(A.shape[0], device=self.device)\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def _cosine_similarity(self, A: t.Tensor, B: t.Tensor, Q: t.Tensor) -> float:\n",
    "        B_transformed = Q @ B\n",
    "        similarity_per_token = F.cosine_similarity(A, B_transformed, dim=0)\n",
    "        return similarity_per_token.mean().item()\n",
    "    \n",
    "\n",
    "    def _create_similarity_matrix(self, caches: t.Tensor) -> t.Tensor:\n",
    "        H = caches.shape[0]\n",
    "        sim_matrix = t.zeros(H, H, device=caches.device) \n",
    "        normalized_caches = caches.clone()\n",
    "        \n",
    "        # normalize each (d_h x N) slice along the d_h dimension\n",
    "        for i in range(H): normalized_caches[i, :, :] = F.normalize(caches[i, :, :], p=2, dim=0)\n",
    "\n",
    "        # create similarity matrix\n",
    "        for i in range(H):\n",
    "            for j in range(i + 1, H):\n",
    "                A = normalized_caches[i]\n",
    "                B = normalized_caches[j]\n",
    "                Q = self._orthogonal_transform(A, B)\n",
    "                sim_value = self._cosine_similarity(A, B, Q)\n",
    "                sim_matrix[i, j] = sim_value\n",
    "                sim_matrix[j, i] = sim_value\n",
    "        return sim_matrix\n",
    "    \n",
    "\n",
    "    def _compute_grouping_score(self, grouping: list[list[int]], sim_matrix: t.Tensor) -> float:\n",
    "        total_score = 0.0\n",
    "        for group in grouping:\n",
    "            for i_idx in range(len(group)):\n",
    "                for j_idx in range(i_idx + 1, len(group)):\n",
    "                    i = group[i_idx]\n",
    "                    j = group[j_idx]\n",
    "                    total_score += sim_matrix[i, j].item()\n",
    "        num_pairs = sum([len(g) * (len(g) - 1) // 2 for g in grouping])\n",
    "        return total_score / num_pairs if num_pairs > 0 else 0.0\n",
    "\n",
    "\n",
    "    def simulate_grouping(self, H: int, D: int, sim_matrix: t.Tensor, max_iter: int) -> tuple[float, list[list[int]]]:\n",
    "        G = H // D\n",
    "        heads = list(range(H))\n",
    "        random.shuffle(heads)\n",
    "        current_grouping = [heads[i * D:(i + 1) * D] for i in range(G)]\n",
    "        score_current = self._compute_grouping_score(current_grouping, sim_matrix)\n",
    "        score_best = score_current\n",
    "        best_grouping = current_grouping\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            # select two different groups for swapping\n",
    "            g1_idx, g2_idx = random.sample(range(G), 2)\n",
    "            \n",
    "            # select a head index within each group\n",
    "            h1_idx, h2_idx = random.randrange(D), random.randrange(D)\n",
    "            \n",
    "            # create groups\n",
    "            new_grouping = [g[:] for g in current_grouping]\n",
    "            \n",
    "            # swap the two heads\n",
    "            h1 = new_grouping[g1_idx][h1_idx]\n",
    "            h2 = new_grouping[g2_idx][h2_idx]\n",
    "            new_grouping[g1_idx][h1_idx] = h2\n",
    "            new_grouping[g2_idx][h2_idx] = h1\n",
    "            \n",
    "            # calc score\n",
    "            score_new = self._compute_grouping_score(new_grouping, sim_matrix)\n",
    "\n",
    "            # accept the swap if the score improves (or equals, for simulated annealing)\n",
    "            if score_new >= score_current:\n",
    "                current_grouping = new_grouping\n",
    "                score_current = score_new\n",
    "                if score_new > score_best:\n",
    "                    score_best = score_new\n",
    "                    best_grouping = new_grouping\n",
    "\n",
    "        return score_best, best_grouping\n",
    "\n",
    "\n",
    "    def find_optimal_n_groups(self, H: int, V_caches: t.Tensor, max_iter_per_G: int) -> dict:\n",
    "        print(f\"... calculating similarity matrix for {H} heads ...\")\n",
    "        \n",
    "        # create similarity matrix\n",
    "        sim_matrix = self._create_similarity_matrix(V_caches.to(self.device))\n",
    "\n",
    "        # list up possible dimension sizes\n",
    "        D_options = [d for d in range(1, H + 1) if H % d == 0]\n",
    "\n",
    "        # start searching \n",
    "        best_overall_score = -float('inf')\n",
    "        optimal_N_GROUPS = H\n",
    "        optimal_N_QUERIES = 1\n",
    "        best_grouping_A = None\n",
    "        print(f\"... testing possible group sizes D's ...\")\n",
    "        for D in D_options:\n",
    "            G = H // D\n",
    "            \n",
    "            # case 1. G = 1 - MQA - one group\n",
    "            if G == 1:\n",
    "                grouping = [list(range(H))]\n",
    "                # use the static method to compute the score\n",
    "                score = self._compute_grouping_score(grouping, sim_matrix)\n",
    "                print(f\"   -> G={G} (D={D}): mqa with a single group. score = {score:.4f}\")\n",
    "\n",
    "            # case 2. D = 1  - MHA - trivial grouping, score is 0.0\n",
    "            elif D == 1:\n",
    "                score = 0.0\n",
    "                grouping = [[i] for i in range(H)]\n",
    "                print(f\"   -> G={G} (D={D}): mha with max groups. score = {score:.4f}\")\n",
    "            \n",
    "            # case 3. G >= 2 - GQA - sim. best gr\n",
    "            elif G >= 2:\n",
    "                score, grouping = self.simulate_grouping(H=H, D=D, sim_matrix=sim_matrix, max_iter=max_iter_per_G)\n",
    "                print(f\"   -> G={G} (D={D}): Best Sim Score (SA) = {score:.4f}\")\n",
    "\n",
    "            # update the optimal config\n",
    "            if score > best_overall_score:\n",
    "                best_overall_score = score\n",
    "                optimal_N_GROUPS = G\n",
    "                optimal_N_QUERIES = D\n",
    "                best_grouping_A = grouping\n",
    "                \n",
    "        return {\n",
    "            'optimal_N_GROUPS': optimal_N_GROUPS,\n",
    "            'optimal_N_QUERIES': optimal_N_QUERIES,\n",
    "            'max_score': best_overall_score,\n",
    "            'best_grouping': best_grouping_A\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "INITIAL_N_GROUPS = 4\n",
    "INITIAL_N_QUERIES = 2 \n",
    "MAX_ITER_PER_G = 300\n",
    "D_V = 64\n",
    "H = 8\n",
    "D_MODEL = D_V * H\n",
    "SEQ_LEN = N_TEST\n",
    "\n",
    "print(f\"... model config: H={H}, D_MODEL={D_MODEL}, D_V={D_V} ...\")\n",
    "print(f\"... initial gqa config: G={INITIAL_N_GROUPS}, D={INITIAL_N_QUERIES} ...\")\n",
    "\n",
    "# instantiate gqa transformer\n",
    "t_gqa = Transformer(\n",
    "    attention_module=GQA(d_model=D_MODEL, d_V=D_V, n_groups=INITIAL_N_GROUPS, n_queries=INITIAL_N_QUERIES),\n",
    "    d_model=D_MODEL,\n",
    "    max_seq_len=N,\n",
    "    tokenizer=TOKENIZER,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# simulate v caches - shape: [H, d_h, N]\n",
    "V_caches_init = t.randn(H, D_V, SEQ_LEN) * 0.1\n",
    "\n",
    "# introduce strong grouping bias to ensure finding a non-trivial solution\n",
    "V_caches_init[0:4] += t.randn(1, D_V, SEQ_LEN) * 0.8 # bias heads 0-3 together, and heads 4-7 together\n",
    "V_caches_init[4:8] += t.randn(1, D_V, SEQ_LEN) * 0.8 \n",
    "\n",
    "# execute optimization method\n",
    "optimal_result = t_gqa.find_optimal_n_groups(H=H, V_caches=V_caches_init, max_iter_per_G=MAX_ITER_PER_G)\n",
    "\n",
    "optimal_n_groups = optimal_result['optimal_N_GROUPS']\n",
    "optimal_n_queries = optimal_result['optimal_N_QUERIES']\n",
    "\n",
    "print(f\"\\n... optimal gqa structure found:\")\n",
    "print(f\"  - optimal groups: {optimal_n_groups}\")\n",
    "print(f\"  - optimal group size: {optimal_n_queries}\")\n",
    "print(f\"  - maximum similarity score: {optimal_result['max_score']:.4f}\")\n",
    "print(f\"  - best grouping (head indices):\\n      {optimal_result['best_grouping']}\")\n",
    "\n",
    "# check if the optimization found the expected 2-group bias\n",
    "if optimal_result['optimal_N_GROUPS'] == 2 and optimal_result['optimal_N_QUERIES'] == 4:\n",
    "    print(\"\\nconclusion: optimization successfully found the strong 2-group bias.\")\n",
    "else:\n",
    "    print(\"\\nconclusion: the optimal grouping was found by simulated annealing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c88f6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=N) # pre-trained tokenizer\n",
    "D_MODEL = 512\n",
    "D_V = 64\n",
    "H = 8\n",
    "N_QUERIES = 8\n",
    "\n",
    "def instantiate_transformers(N: int) -> list[Transformer]:\n",
    "    t_baseline = Transformer(\n",
    "        attention_module=StandardAttention(d_model=D_MODEL, d_V=D_V),\n",
    "        d_model=D_MODEL,\n",
    "        max_seq_len=N,\n",
    "        tokenizer=TOKENIZER,\n",
    "    )\n",
    "    t_mha = Transformer(\n",
    "        attention_module=MHA(d_model=D_MODEL, d_V=D_V, H=H),\n",
    "        d_model=D_MODEL,\n",
    "        max_seq_len=N,\n",
    "        tokenizer=TOKENIZER,\n",
    "    )\n",
    "    t_mqa = Transformer(\n",
    "        attention_module=MQA(d_model=D_MODEL, d_V=D_V, n_queries=N_QUERIES),\n",
    "        d_model=D_MODEL,\n",
    "        max_seq_len=N,\n",
    "        tokenizer=TOKENIZER,\n",
    "    )\n",
    "    t_gqa = Transformer(\n",
    "        attention_module=GQA(d_model=D_MODEL, d_V=D_V, n_groups=optimal_n_groups, n_queries=optimal_n_queries),\n",
    "        d_model=D_MODEL,\n",
    "        max_seq_len=N,\n",
    "        tokenizer=TOKENIZER,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    return [t_baseline, t_mha, t_mqa, t_gqa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e82aa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "import time\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(\n",
    "        model: Transformer,\n",
    "        train_data_loader: DataLoader,\n",
    "        val_data_loader: DataLoader,\n",
    "        criterion: nn.Module = nn.CrossEntropyLoss(ignore_index=TOKENIZER.pad_token_id),\n",
    "        num_epochs: int = 10,\n",
    "        lr: float = 1e-4,\n",
    "        min_delta: float = 1e-3,\n",
    "        patience: int = 5,\n",
    "        device: t.device = DEVICE,\n",
    "    ) -> float:\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # training loop\n",
    "    min_val_loss, counter = float('inf'), 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, target_labels = model(inputs, labels)\n",
    "            logits = logits.view(-1, logits.size(-1)) # reshape logits: [B, L, V] -> [B*L, V]\n",
    "            target_labels = target_labels.reshape(-1) # reshape target_labels: [B, L] -> [B*L]\n",
    "\n",
    "            loss = criterion(logits, target_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"... epoch {epoch+1}/{num_epochs}, batch {i+1} (train), loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                logits, target_labels = model(inputs, labels)\n",
    "                logits = logits.view(-1, logits.size(-1)) # reshape logits: [B, L, V] -> [B*L, V]\n",
    "                target_labels = target_labels.reshape(-1)  # reshape target_labels: [B, L] -> [B*L]\n",
    "\n",
    "                # pass reshaped tensors to the criterion\n",
    "                loss = criterion(logits, target_labels)\n",
    "                val_loss += loss.item() * target_labels.size(0)\n",
    "                total_val_samples += target_labels.size(0)\n",
    "          \n",
    "        avg_val_loss = val_loss / total_val_samples\n",
    "        epoch_end_time = time.time()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"... epoch {epoch+1} - avg val loss: {avg_val_loss:.4f}, time: {epoch_end_time - epoch_start_time:.2f}s ...\")\n",
    "        \n",
    "        # early stopping\n",
    "        if avg_val_loss < min_val_loss - min_delta:\n",
    "            min_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "        elif avg_val_loss >= min_val_loss + min_delta:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"... early stopping triggered for {model.model_name} after {epoch + 1} epochs (patience={patience}) ...\")\n",
    "                break\n",
    "        \n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    print(f\"... training of {model.model_name} successfully concluded in {total_duration:.2f} seconds ...\")\n",
    "\n",
    "    return min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63449d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST - N=512, B=1\n",
      "\n",
      "Model: StandardAttention\n",
      "... epoch 10 - avg val loss: 44.7783, time: 0.30s ...\n",
      "... epoch 20 - avg val loss: 43.1605, time: 0.28s ...\n",
      "... early stopping triggered for StandardAttention after 20 epochs (patience=5) ...\n",
      "... training of StandardAttention successfully concluded in 5.80 seconds ...\n",
      "min. val loss: 42.4250\n",
      "decoding latency: 0.1330 s\n",
      "\n",
      "\n",
      "Model: MHA\n",
      "... epoch 10 - avg val loss: 45.8643, time: 0.33s ...\n",
      "... epoch 20 - avg val loss: 43.6215, time: 0.35s ...\n",
      "... early stopping triggered for MHA after 21 epochs (patience=5) ...\n",
      "... training of MHA successfully concluded in 7.10 seconds ...\n",
      "min. val loss: 42.6592\n",
      "decoding latency: 2.8384 s\n",
      "\n",
      "\n",
      "Model: MQA\n",
      "... epoch 10 - avg val loss: 47.8086, time: 0.33s ...\n",
      "... epoch 20 - avg val loss: 46.4194, time: 0.33s ...\n",
      "... epoch 30 - avg val loss: 45.6682, time: 0.32s ...\n",
      "... early stopping triggered for MQA after 38 epochs (patience=5) ...\n",
      "... training of MQA successfully concluded in 12.29 seconds ...\n",
      "min. val loss: 45.5212\n",
      "decoding latency: 2.6734 s\n",
      "\n",
      "\n",
      "Model: GQA\n",
      "... epoch 10 - avg val loss: 49.2987, time: 0.35s ...\n",
      "... epoch 20 - avg val loss: 47.6060, time: 0.35s ...\n",
      "... early stopping triggered for GQA after 25 epochs (patience=5) ...\n",
      "... training of GQA successfully concluded in 8.77 seconds ...\n",
      "min. val loss: 47.6060\n",
      "decoding latency: 2.9353 s\n",
      "\n",
      "TEST - N=512, B=4\n",
      "\n",
      "Model: StandardAttention\n",
      "... epoch 10 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 20 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 30 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 40 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 50 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 60 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 70 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 80 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 90 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... epoch 100 - avg val loss: 56.2510, time: 0.02s ...\n",
      "... training of StandardAttention successfully concluded in 2.21 seconds ...\n",
      "min. val loss: 56.2510\n",
      "decoding latency: 0.5238 s\n",
      "\n",
      "\n",
      "Model: MHA\n",
      "... epoch 10 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 51.5783, time: 0.04s ...\n",
      "... epoch 40 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 51.5783, time: 0.03s ...\n",
      "... training of MHA successfully concluded in 3.06 seconds ...\n",
      "min. val loss: 51.5783\n",
      "decoding latency: 2.7774 s\n",
      "\n",
      "\n",
      "Model: MQA\n",
      "... epoch 10 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 51.6683, time: 0.03s ...\n",
      "... training of MQA successfully concluded in 2.57 seconds ...\n",
      "min. val loss: 51.6683\n",
      "decoding latency: 2.4841 s\n",
      "\n",
      "\n",
      "Model: GQA\n",
      "... epoch 10 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 54.2173, time: 0.03s ...\n",
      "... training of GQA successfully concluded in 2.79 seconds ...\n",
      "min. val loss: 54.2173\n",
      "decoding latency: 2.8672 s\n",
      "\n",
      "TEST - N=512, B=8\n",
      "\n",
      "Model: StandardAttention\n",
      "... epoch 10 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 20 - avg val loss: 51.0474, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 40 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 50 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 60 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 70 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 80 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 90 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... epoch 100 - avg val loss: 51.0474, time: 0.02s ...\n",
      "... training of StandardAttention successfully concluded in 2.43 seconds ...\n",
      "min. val loss: 51.0474\n",
      "decoding latency: 2.0127 s\n",
      "\n",
      "\n",
      "Model: MHA\n",
      "... epoch 10 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 49.8853, time: 0.03s ...\n",
      "... training of MHA successfully concluded in 2.86 seconds ...\n",
      "min. val loss: 49.8853\n",
      "decoding latency: 2.8398 s\n",
      "\n",
      "\n",
      "Model: MQA\n",
      "... epoch 10 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 56.7827, time: 0.03s ...\n",
      "... training of MQA successfully concluded in 3.06 seconds ...\n",
      "min. val loss: 56.7827\n",
      "decoding latency: 2.6173 s\n",
      "\n",
      "\n",
      "Model: GQA\n",
      "... epoch 10 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 50.4768, time: 0.03s ...\n",
      "... training of GQA successfully concluded in 3.04 seconds ...\n",
      "min. val loss: 50.4768\n",
      "decoding latency: 3.0962 s\n",
      "\n",
      "TEST - N=512, B=16\n",
      "\n",
      "Model: StandardAttention\n",
      "... epoch 10 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 20 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 30 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 40 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 50 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 60 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 70 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 80 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 90 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... epoch 100 - avg val loss: 55.4537, time: 0.02s ...\n",
      "... training of StandardAttention successfully concluded in 2.26 seconds ...\n",
      "min. val loss: 55.4537\n",
      "decoding latency: 2.0784 s\n",
      "\n",
      "\n",
      "Model: MHA\n",
      "... epoch 10 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 48.2117, time: 0.03s ...\n",
      "... training of MHA successfully concluded in 2.84 seconds ...\n",
      "min. val loss: 48.2117\n",
      "decoding latency: 2.7416 s\n",
      "\n",
      "\n",
      "Model: MQA\n",
      "... epoch 10 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... epoch 70 - avg val loss: 50.4389, time: 0.02s ...\n",
      "... epoch 80 - avg val loss: 50.4389, time: 0.02s ...\n",
      "... epoch 90 - avg val loss: 50.4389, time: 0.02s ...\n",
      "... epoch 100 - avg val loss: 50.4389, time: 0.03s ...\n",
      "... training of MQA successfully concluded in 2.64 seconds ...\n",
      "min. val loss: 50.4389\n",
      "decoding latency: 2.5617 s\n",
      "\n",
      "\n",
      "Model: GQA\n",
      "... epoch 10 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 20 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 30 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 40 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 50 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 60 - avg val loss: 55.0251, time: 0.04s ...\n",
      "... epoch 70 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 80 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 90 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... epoch 100 - avg val loss: 55.0251, time: 0.03s ...\n",
      "... training of GQA successfully concluded in 2.85 seconds ...\n",
      "min. val loss: 55.0251\n",
      "decoding latency: 2.8366 s\n",
      "\n",
      "TEST - N=1024, B=1\n",
      "\n",
      "Model: StandardAttention\n",
      "... epoch 10 - avg val loss: 47.9846, time: 0.51s ...\n",
      "... epoch 20 - avg val loss: 46.7540, time: 0.48s ...\n",
      "... epoch 30 - avg val loss: 46.5817, time: 0.48s ...\n",
      "... early stopping triggered for StandardAttention after 31 epochs (patience=5) ...\n",
      "... training of StandardAttention successfully concluded in 15.13 seconds ...\n",
      "min. val loss: 46.3109\n",
      "decoding latency: 7.8837 s\n",
      "\n",
      "\n",
      "Model: MHA\n",
      "... epoch 10 - avg val loss: 46.4016, time: 0.64s ...\n",
      "... epoch 20 - avg val loss: 45.3658, time: 0.63s ...\n",
      "... early stopping triggered for MHA after 20 epochs (patience=5) ...\n",
      "... training of MHA successfully concluded in 12.57 seconds ...\n",
      "min. val loss: 44.3614\n",
      "decoding latency: 13.4203 s\n",
      "\n",
      "\n",
      "Model: MQA\n",
      "... epoch 10 - avg val loss: 49.8570, time: 0.66s ...\n",
      "... epoch 20 - avg val loss: 46.6100, time: 0.61s ...\n",
      "... early stopping triggered for MQA after 27 epochs (patience=5) ...\n",
      "... training of MQA successfully concluded in 16.91 seconds ...\n",
      "min. val loss: 45.9546\n"
     ]
    }
   ],
   "source": [
    "# func to measure ave. inf latency \n",
    "\n",
    "import torch as t\n",
    "\n",
    "\n",
    "def test_decoding_latency(model: Transformer, X_input: t.Tensor, N: int, num_runs: int = 30) -> float:\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    for _ in range(3): model.generate(X_input, max_length=N)\n",
    "    if DEVICE.type == 'cuda': t.cuda.synchronize() # wait for gpu to finish\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs): model.generate(X_input, max_length=N) \n",
    "\n",
    "    if DEVICE.type == 'cuda': t.cuda.synchronize() # wait for gpu to finish\n",
    "    end_time = time.time()\n",
    "    avg_time_ms = ((end_time - start_time) / num_runs) * 1000\n",
    "    return avg_time_ms\n",
    "\n",
    "\n",
    "# exp - define the variables for comparison\n",
    "N_VALUES = [512, 1024, 2048]\n",
    "B_VALUES = [1, 4, 8, 16]\n",
    "NUM_VAL_SAMPLES = 512\n",
    "results = list() # for recodring\n",
    "\n",
    "\n",
    "for N in N_VALUES:\n",
    "    # tokenize data\n",
    "    tokenized_source = TOKENIZER(en_raw, padding='max_length', truncation=True, max_length=N, return_tensors='pt')\n",
    "    X = tokenized_source['input_ids'].to(DEVICE)\n",
    "\n",
    "    tokenized_target = TOKENIZER(fr_raw, padding='max_length', truncation=True, max_length=N, return_tensors='pt')\n",
    "    y = tokenized_target['input_ids'].to(DEVICE)\n",
    "\n",
    "    # create tensor data\n",
    "    full_dt = TensorDataset(X, y)\n",
    "    \n",
    "    # split dt - 80% train, 20% val\n",
    "    train_size = int(0.8 * len(full_dt))\n",
    "    val_size = len(full_dt) - train_size\n",
    "    generator = t.Generator().manual_seed(42) # for reproducible splits\n",
    "    train_dt, val_dt = random_split(full_dt, [train_size, val_size], generator=generator)\n",
    "\n",
    "    for B in B_VALUES:\n",
    "        print(f\"TEST - N={N}, B={B}\")\n",
    "        \n",
    "        # instantiate models\n",
    "        models = instantiate_transformers(N=N) \n",
    "        \n",
    "        # create data loader\n",
    "        train_data_loader = DataLoader(train_dt, batch_size=B, shuffle=True, drop_last=True)\n",
    "        val_data_loader = DataLoader(val_dt, batch_size=B, shuffle=False, drop_last=False)\n",
    "        \n",
    "        # get a sample batch for the latency test\n",
    "        X_sample, _ = val_dt[0: B] # a sample batch of size B\n",
    "        X_sample = X_sample.to(DEVICE)\n",
    "\n",
    "        for model in models:\n",
    "            model_name = model.model_name\n",
    "            print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "            # metrics 1: performance - val loss\n",
    "            val_loss = train_model(model=model, train_data_loader=train_data_loader, val_data_loader=val_data_loader, num_epochs=100)\n",
    "            print(f\"min. val loss: {val_loss:,.4f}\")\n",
    "\n",
    "            # metrics 2. inference latency\n",
    "            latency = test_decoding_latency(model, X_sample, N=N)\n",
    "            print(f\"decoding latency: {latency/1000:,.4f} sec\\n\")\n",
    "            \n",
    "            # store the result\n",
    "            results.append(dict(model=model_name, N=N, B=B, latency_ms=latency, val_loss=val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = [model.model_name for model in models]\n",
    "colors = ['black', 'blue', 'green', 'darkred']\n",
    "# line_styles = ['-', '--', ':', '-.']\n",
    "line_styles = ['-', '-', '-', '-',]\n",
    "\n",
    "def plot_metric(data, metric_key, metric_label, B_VALUES):\n",
    "    fig, axes = plt.subplots(1, len(B_VALUES), figsize=(5 * len(B_VALUES), 6), sharey=True)\n",
    "    \n",
    "    if len(B_VALUES) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, B in enumerate(B_VALUES):\n",
    "        ax = axes[i]\n",
    "        B_data = [r for r in data if r['B'] == B]\n",
    "        \n",
    "        for j, model_name in enumerate(model_names):\n",
    "            model_data = sorted([r for r in B_data if r['model'] == model_name], key=lambda x: x['N'])\n",
    "            N_list = [r['N'] for r in model_data]\n",
    "            metric_list = [r[metric_key] for r in model_data]\n",
    "            \n",
    "            ax.plot(N_list, metric_list, marker='o', linestyle=line_styles[j % len(line_styles)], color=colors[j], label=model_name)\n",
    "        \n",
    "        ax.set_title(f'Batch Size (B) = {B}')\n",
    "        ax.set_xlabel('Sequence Length (N)')\n",
    "        ax.set_xticks(N_VALUES)\n",
    "        ax.set_xticklabels([str(n) for n in N_VALUES])\n",
    "        ax.grid(True, which='both', linestyle='--')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel(metric_label)\n",
    "\n",
    "    handles, labels = axes[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=len(model_names))\n",
    "    \n",
    "    fig.suptitle(f'Model Comparison: {metric_label} vs. Sequence Length (N)', y=1.10)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]) # type: ignore\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metric(results, 'latency_ms', 'Inference Latency (ms)', B_VALUES)\n",
    "plot_metric(results, 'val_loss', 'Validation Loss (Lower is Better)', B_VALUES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
